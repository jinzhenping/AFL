import os
import time
import argparse
import json
import jsonlines

from tqdm import tqdm
import random
from torch.utils.data import Dataset, DataLoader
import multiprocessing
import sys
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from dataset.lastfm_dataset import LastfmDataset
from dataset.mind_dataset import MindDataset

from utils.regular_function import split_user_response, split_rec_reponse
from utils.rw_process import append_jsonl
from utils.agent import RecAgent, UserModelAgent
from utils.model import SASRec
finish_num = 0
total = 0
correct = 0
def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_dir', type=str, default='')
    parser.add_argument('--model_path', type=str, default=None, help='Path to SASRec model. For MIND dataset, defaults to ./SASRec.pth if not specified.')
    parser.add_argument('--prior_file', type=str, default='')
    parser.add_argument('--stage', type=str, default='test')
    parser.add_argument('--cans_num', type=int, default=20)
    parser.add_argument('--sep', type=str, default=', ')
    parser.add_argument('--max_epoch', type=int, default=1)
    parser.add_argument('--output_file', type=str, required=True, help='Output file path for evaluation results (JSONL format)')
    parser.add_argument('--model', type=str, default='')
    parser.add_argument('--api_key', type=str)
    parser.add_argument('--max_retry_num', type=int, default=5)
    parser.add_argument('--seed', type=int, default=333)
    parser.add_argument('--mp', type=int, default=1)
    parser.add_argument('--temperature', type=float, default=0.0)
    parser.add_argument("--save_info",action="store_true")
    parser.add_argument("--save_rec_dir", type=str, default=None)
    parser.add_argument("--save_user_dir", type=str, default=None)
    parser.add_argument("--gpu", type=int, default=None, help='GPU device ID to use (e.g., 0 or 1). If not specified, uses CUDA_VISIBLE_DEVICES or default cuda device.')
    return parser.parse_args()

def recommend(data, args):
    start_time = time.time()
    rec_agent = RecAgent(args,'prior_rec')
    user_agent = UserModelAgent(args,'prior_rec')
    flag = False
    epoch = 1
    rec_item = None
    new_data_list = []
    while flag == False and epoch <= args.max_epoch:
        #rec agent
        while True:
            rec_agent_response = rec_agent.act(data)
            rec_reason, rec_item = split_rec_reponse(rec_agent_response)
            if rec_item is not None:
                break
        if args.max_epoch == 1:
            new_data = {'id':data['id'],'seq_name':data['seq_name'], 'cans_name':data['cans_name'], 'correct_answer':data['correct_answer'], 'epoch':epoch, 'rec_res':rec_agent_response, 'user_res':None,'prior_answer':data['prior_answer']}
            new_data_list.append(new_data)
            
            memory_info = {"epoch":epoch, "rec_reason":rec_reason, "rec_item":rec_item, "user_reason":None}
            rec_agent.update_memory(memory_info)
            user_agent.update_memory(memory_info)
            break
        
        #user agent
        while True:
            user_agent_response = user_agent.act(data,rec_reason, rec_item)
            user_reason, flag  = split_user_response(user_agent_response)
            if flag is not None:
                break

        # save
        new_data = {'id':data['id'],'seq_name':data['seq_name'], 'cans_name':data['cans_name'], 'correct_answer':data['correct_answer'], 'epoch':epoch, 'rec_res':rec_agent_response, 'user_res':user_agent_response,'prior_answer':data['prior_answer']}
        new_data_list.append(new_data)

        memory_info = {"epoch":epoch, "rec_reason":rec_reason, "rec_item":rec_item, "user_reason":user_reason}
        rec_agent.update_memory(memory_info)
        user_agent.update_memory(memory_info)
        #update 
        if flag:
            break

        epoch += 1
    end_time = time.time()
    print("recommendation time = ", end_time - start_time)
    # save
    if args.save_info:
        rec_file_path = os.path.join(args.save_rec_dir, f"{data['id']}.jsonl")
        user_file_path = os.path.join(args.save_user_dir, f"{data['id']}.jsonl")
        rec_agent.save_memory(rec_file_path)
        user_agent.save_memory(user_file_path)
    # evaluate
    if rec_item.lower() == data['correct_answer'].lower().strip():
        return new_data_list, 1, args
    else:
        return new_data_list, 0, args

def setcallback(x):
    global finish_num
    global total
    global correct

    data_list, flag, args = x
    for data in data_list:
        append_jsonl(args.output_file, data)
    finish_num += 1
    correct += flag
    print("==============")
    print(f"current hit@1 = {correct} / {finish_num} = {correct/finish_num}")
    print(f"global hit@1 = {correct} / {total} = {correct/total}")
    print("==============")

def main(args):
    if args.output_file is None:
        raise ValueError("--output_file is required. Please specify the output file path.")
    
    if args.save_info and args.save_rec_dir is not None and not os.path.exists(args.save_rec_dir):
        os.makedirs(args.save_rec_dir)
    if args.save_info and args.save_user_dir is not None and not os.path.exists(args.save_user_dir):
        os.makedirs(args.save_user_dir)
    if os.path.exists(args.output_file):
        os.remove(args.output_file)
    if 'lastfm' in args.data_dir:
        dataset = LastfmDataset(args.data_dir, args.stage, args.cans_num, args.sep, True)
    elif 'mind' in args.data_dir.lower():
        dataset = MindDataset(args.data_dir, args.stage, args.cans_num, args.sep, True)
    else:
        raise ValueError(f"Unsupported dataset: {args.data_dir}")
    global total
    data_list = []
    print("data size = ", total)
    for data in tqdm(dataset):
        data_list.append(data)
    import pandas as pd
    prior_df = pd.read_csv(args.prior_file)
    prior_list = prior_df.to_dict('records')
    prior_dict = {}
    for data in prior_list:
        prior_dict[data['id']] = data

    merge_data_list = []
    for data in data_list:
        generate = prior_dict[data['id']]['generate']
        merge_data = data.copy()
        merge_data['prior_answer'] = generate
        merge_data_list.append(merge_data)
    pool = multiprocessing.Pool(args.mp)
    total = len(merge_data_list)
    for data in tqdm(merge_data_list):
        pool.apply_async(func=recommend, args=(data, args), callback=setcallback)
    pool.close()
    pool.join()

if __name__ == '__main__':
    args = get_args()
    random.seed(args.seed)
    main(args)
    